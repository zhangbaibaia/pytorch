{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "断言语句和 if 分支有点类似，它用于对一个 bool 表达式进行断言，如果该 bool 表达式为 True，该程序可以继续向下执行；否则程序会引发 AssertionError 错误。\n",
    "\n",
    "断言格式 ： assert+空格+要判断语句+双引号“报错语句”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()\n",
    "    \n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16).view(2, 8)\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  4.,  0.,  0., 10.,  0., 14.],\n",
       "        [ 0., 18., 20.,  0.,  0.,  0., 28., 30.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isinstance(object, classinfo)\n",
    "\n",
    "参数:\n",
    "object -- 实例对象。\n",
    "classinfo -- 可以是直接或间接类名、基本类型或者由它们组成的元组。\n",
    "\n",
    "返回值:如果对象的类型与参数二的类型（classinfo）相同则返回 True，否则返回 False。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化\n",
    "如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier随机初始化\n",
    "它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "#torch.nn.inti.normal_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考虑环境因素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协变量偏移：不考虑适应新的情况、根源在于特征分布的变化\n",
    "\n",
    "标签偏移： 导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）\n",
    "\n",
    "概念偏移： 概念转换，即标签本身的定义发生变化的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('E:\\jupyter\\股票\\PEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>证券代码</th>\n",
       "      <th>会计期间</th>\n",
       "      <th>报表类型</th>\n",
       "      <th>净利润</th>\n",
       "      <th>time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>净利润(t-1)</th>\n",
       "      <th>净利润增长率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>A</td>\n",
       "      <td>7374678.80</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>2005-06-30</td>\n",
       "      <td>A</td>\n",
       "      <td>22888418.26</td>\n",
       "      <td>2005-06-30</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>7374678.80</td>\n",
       "      <td>2.103650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>A</td>\n",
       "      <td>41089276.40</td>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>22888418.26</td>\n",
       "      <td>0.795199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>51217257.93</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>41089276.40</td>\n",
       "      <td>0.246487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>2006-03-31</td>\n",
       "      <td>A</td>\n",
       "      <td>15590633.98</td>\n",
       "      <td>2006-03-31</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>51217257.93</td>\n",
       "      <td>-0.695598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   证券代码        会计期间 报表类型          净利润        time  year  month     净利润(t-1)  \\\n",
       "0  2007  2005-03-31    A   7374678.80  2005-03-31  2005      1         0.00   \n",
       "1  2007  2005-06-30    A  22888418.26  2005-06-30  2005      2   7374678.80   \n",
       "2  2007  2005-09-30    A  41089276.40  2005-09-30  2005      3  22888418.26   \n",
       "3  2007  2005-12-31    A  51217257.93  2005-12-31  2005      4  41089276.40   \n",
       "4  2007  2006-03-31    A  15590633.98  2006-03-31  2006      1  51217257.93   \n",
       "\n",
       "     净利润增长率  \n",
       "0       inf  \n",
       "1  2.103650  \n",
       "2  0.795199  \n",
       "3  0.246487  \n",
       "4 -0.695598  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peg=pd.read_csv('lirun_1.csv',encoding='utf_8_sig')\n",
    "peg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_peg=peg.dtypes[peg.dtypes != 'object'].index\n",
    "peg[num_peg] = peg[num_peg].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值\n",
    "peg[num_peg] = peg[num_peg].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    # 返回第i折交叉验证时所需要的训练和验证数据\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat((X_train, X_part), dim=0)\n",
    "            y_train = torch.cat((y_train, y_part), dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net(X_train.shape[1])\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n",
    "                         range(1, num_epochs + 1), valid_ls,\n",
    "                         ['train', 'valid'])\n",
    "        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的感受野（receptive field）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。\n",
    "\n",
    "NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器翻译相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要特征：输出时单词序列而不是一个单词，实处的序列长度可能与原序列的长度不同\n",
    "\n",
    "对于短序列，用其他字符补全\n",
    "对于长序列，截断（max_len）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数（不考虑pad）\n",
    "def SequenceMask(X, X_len,value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n",
    "    X[~mask]=value\n",
    "    return X\n",
    "\n",
    "import torch.nn as nn\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # pred shape: (batch_size, seq_len, vocab_size)\n",
    "    # label shape: (batch_size, seq_len)\n",
    "    # valid_length shape: (batch_size, )\n",
    "    def forward(self, pred, label, valid_length):\n",
    "        # the sample weights shape should be (batch_size, seq_len)\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = SequenceMask(weights, valid_length).float()\n",
    "        self.reduction='none'\n",
    "        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n",
    "        return (output*weights).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3], [4,5,6]])\n",
    "SequenceMask(X,torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.7269, 0.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维特比算法：选择整体分数最高的句子（搜索空间太大） 集束搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(5,dtype=torch.float)\n",
    "#mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "tensor([[0., 1., 2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a[None, :])   #增加一个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name_walk(file_dir):\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        print(\"root\", root)  # 当前目录路径\n",
    "        print(\"dirs\", dirs)  # 当前路径下所有子目录\n",
    "        print(\"files\", files)  # 当前路径下所有非目录子文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力得分：点积注意力\n",
    "class DotProductAttention(nn.Module): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # valid_length: either (batch_size, ) or (batch_size, xx)\n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        \n",
    "        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        print(\"attention_weight\\n\",attention_weights)\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 操作符的屏蔽操作\n",
    "def SequenceMask(X, X_len,value=-1e6):\n",
    "    maxlen = X.size(1)\n",
    "    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n",
    "    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n",
    "    #print(mask)\n",
    "    X[mask]=value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X, valid_length):\n",
    "    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    if valid_length is None:\n",
    "        return softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_length.dim() == 1:\n",
    "            try:\n",
    "                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "            except:\n",
    "                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "        else:\n",
    "            valid_length = valid_length.reshape((-1,))\n",
    "        # fill masked elements with a large negative, whose exp is 0\n",
    "        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n",
    " \n",
    "        return softmax(X).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weight\n",
      " tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "atten = DotProductAttention(dropout=0)\n",
    "\n",
    "keys = torch.ones((2,10,2),dtype=torch.float)\n",
    "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
    "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层感知机注意力\n",
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, units,ipt_dim,dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.\n",
    "        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.v = nn.Linear(units, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_k(query), self.W_q(key)\n",
    "        #print(\"size\",query.size(),key.size())\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to\n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        #print(\"features:\",features.size())  #--------------开启\n",
    "        scores = self.v(features).squeeze(-1) \n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\n",
    "atten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](png/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置编码用来捕捉序列信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_size, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = np.zeros((1, max_len, embedding_size))\n",
    "        X = np.arange(0, max_len).reshape(-1, 1) / np.power(\n",
    "            10000, np.arange(0, embedding_size, 2)/embedding_size)\n",
    "        self.P[:, :, 0::2] = np.sin(X)\n",
    "        self.P[:, :, 1::2] = np.cos(X)\n",
    "        self.P = torch.FloatTensor(self.P)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if X.is_cuda and not self.P.is_cuda:\n",
    "            self.P = self.P.cuda()\n",
    "        X = X + self.P[:, :X.shape[1], :]\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
